# -*- coding: utf-8 -*-
"""Mnist Dataset Neural Network building_Quiz_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wUqWW3o0Dz5E5tlsCs4uzi2OJQx9tQs
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Activation
import datetime
from tensorflow.keras.callbacks import TensorBoard

'''
LOAD MNIST DATASET FOR MODEL BUILDING ON DATASETS
'''

(x_train, y_train) , (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Convert the record data's into float value
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

## Standardize the image pixel values between (0-1) for faster conversion
x_test = x_test/255
x_train = x_train/255

## in (R,G,B) image sequencing : each value is in the range of (0-255)
## data standardization is important because several heterogenuous categories of data when put together without proper standardization
## may lead to slower/ no convergence and may affect the outcome in a disprportionate manner

unique_classes = np.unique(y_train)
print(unique_classes)

## Here, we are visualizing the encoding of the of each number with their respective output class of the mnist fashion dataset where it corresponds to
## where {0:"T-shirt/top", 1:"Trouser" , 2:"Pullover" , 3:"Dress" , 4:"Coat" , 5:"Sandal" , 6:"Shirt" , 7: "Sneaker" , 8:"Bag" , 9:"Ankle Boot"}
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[y_train[i]])  ## Here bascially the y_train[i] gives a numerical output lying in (0-9) where each digit is mapped to a product
plt.show()

## Model Building stage for the mnist fashion dataset
## input -> output
# (28X28) -> (10)
# Dense : m(1D) -> n(1D)

model = tf.keras.Sequential(
    [
    ## right about the time we are looking to load the data in the ANN we gotta standardize the image dataset
    Flatten(input_shape = (28,28)), ## standardizing the image in 28X28 resolution
    Dense(240, activation = 'relu'),
    Dense(30, activation = 'relu'),
    Dense(20, activation = 'relu'),
    Dense(10, activation = 'sigmoid') ## since we are looking at a multi-class classification sigmoid activation is the best choice for that
    ## the class with the maximum probabilisitic score based on the sigmoid function can is the class that it will be classified into
    ]
)

model.summary()

model.compile(optimizer='adam',
			loss='sparse_categorical_crossentropy',
			metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=18,
										batch_size=275,
										validation_split=0.3)

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure()
plt.plot(history.epoch, loss, 'green', label='Training Loss')
plt.plot(history.epoch, val_loss, 'red', label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

results = model.evaluate(x_test, y_test, verbose = 0)
print('test loss, test acc:', results)

# Function to create model with variable layers and neurons
def create_model(num_hidden_layers=1, num_neurons_per_layer=128):
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    for _ in range(num_hidden_layers):
        model.add(Dense(num_neurons_per_layer, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    return model

# Function to train and evaluate model with TensorBoard logging
def train_and_evaluate_model(num_hidden_layers, num_neurons_per_layer, log_dir, x_train, y_train, x_test, y_test):
    model = create_model(num_hidden_layers, num_neurons_per_layer)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

    history = model.fit(x_train, y_train, epochs=18, batch_size=275, validation_split=0.3, callbacks=[tensorboard_callback])

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    return test_acc, history

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

# Normalize the images
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Setup TensorBoard logging directory
current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
log_dir = "logs/fit/" + current_time

# Ablation study configurations
configurations = [
    {'num_hidden_layers': 1, 'num_neurons_per_layer': 128},
    {'num_hidden_layers': 2, 'num_neurons_per_layer': 128},
    {'num_hidden_layers': 1, 'num_neurons_per_layer': 64},
    {'num_hidden_layers': 2, 'num_neurons_per_layer': 64},
]

# Perform ablation study
results = []
for config in configurations:
    log_subdir = log_dir + f"/layers_{config['num_hidden_layers']}_neurons_{config['num_neurons_per_layer']}"
    test_acc, history = train_and_evaluate_model(config['num_hidden_layers'], config['num_neurons_per_layer'], log_subdir, x_train, y_train, x_test, y_test)
    results.append((config, test_acc))

# Print results
for config, test_acc in results:
    print(f"Config: {config}, Test Accuracy: {test_acc}")



import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import plot_model
import datetime
import os

# Function to create a model with variable layers and neurons
def create_model(num_hidden_layers=1, num_neurons_per_layer=128):
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    for _ in range(num_hidden_layers):
        model.add(Dense(num_neurons_per_layer, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    return model

# Function to log images in TensorBoard
def log_images(writer, images, labels, predictions, step):
    with writer.as_default():
        for i in range(len(images)):
            img = tf.expand_dims(images[i], axis=-1)  # Expand dimensions for single image
            img = tf.image.convert_image_dtype(img, dtype=tf.uint8)  # Convert to uint8
            img = tf.image.encode_png(img)  # Encode as PNG
            img = tf.expand_dims(img, 0)  # Add batch dimension
            tf.summary.image(f"Prediction: {predictions[i]}, Label: {labels[i]}", img, step=step)

# Function to train and evaluate the model with TensorBoard logging
def train_and_evaluate_model(num_hidden_layers, num_neurons_per_layer, log_dir, x_train, y_train, x_test, y_test):
    model = create_model(num_hidden_layers, num_neurons_per_layer)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
    file_writer_images = tf.summary.create_file_writer(log_dir + "/images")

    history = model.fit(x_train, y_train, epochs=18, batch_size=275, validation_split=0.3, callbacks=[tensorboard_callback])

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)

    # Log images to TensorBoard
    sample_images = x_test[:25]  # Take 25 sample images
    sample_labels = y_test[:25]  # Corresponding labels
    predictions = model.predict(sample_images)
    predicted_labels = np.argmax(predictions, axis=1)
    log_images(file_writer_images, sample_images, sample_labels, predicted_labels, step=1)

    # Ensure directory exists before saving the model architecture
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    plot_model(model, to_file=os.path.join(log_dir, 'model.png'), show_shapes=True, show_layer_names=True)

    return test_acc, history

# Load and preprocess the Fashion MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

# Normalize the images
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Setup TensorBoard logging directory
current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
log_dir = "logs/fit/" + current_time

# Ablation study configurations
configurations = [
    {'num_hidden_layers': 1, 'num_neurons_per_layer': 128},
    {'num_hidden_layers': 2, 'num_neurons_per_layer': 128},
    {'num_hidden_layers': 1, 'num_neurons_per_layer': 64},
    {'num_hidden_layers': 2, 'num_neurons_per_layer': 64},
]

# Perform ablation study
results = []
for config in configurations:
    log_subdir = log_dir + f"/layers_{config['num_hidden_layers']}_neurons_{config['num_neurons_per_layer']}"

    # Ensure the subdirectory exists before passing it to train_and_evaluate_model
    if not os.path.exists(log_subdir):
        os.makedirs(log_subdir)

    test_acc, history = train_and_evaluate_model(config['num_hidden_layers'], config['num_neurons_per_layer'], log_subdir, x_train, y_train, x_test, y_test)
    results.append((config, test_acc))

# Print results
for config, test_acc in results:
    print(f"Config: {config}, Test Accuracy: {test_acc}")

# Instructions to run TensorBoard
print("To visualize the training process, run the following command in your terminal:")
print("tensorboard --logdir=logs/fit")

from tensorflow.keras.utils import plot_model

model = create_model()  # Ensure to replace this with your model creation function
plot_model(model, to_file='logs/fit/model.png', show_shapes=True, show_layer_names=True)

